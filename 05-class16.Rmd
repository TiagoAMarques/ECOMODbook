# Class 16: 18 11 2020 {#aula16}

## A logistic regression example

```{r}

#
set.seed(123)
n=200
#a covariate
xs=runif(n,-20,20)
#get the mean value
ilogit=function(x){
  il=exp(x)/(1+exp(x))
return(il)
}

Ey=ilogit(2+0.4*xs)
#generate data
ys=rbinom(n,size = rep(1,n),prob = Ey)
#plot data
plot(xs,ys)


#run a glm
glmLR1=glm(ys~xs,family=binomial(link="logit"))
summary(glmLR1)

xs4pred=seq(min(xs),max(xs),length=100)
predglmLR1=predict(glmLR1,newdata = data.frame(xs=xs4pred),
type="response")

par(mfrow=c(1,1),mar=c(4,4,0.2,0.2))
plot(xs,ys)
lines(xs4pred,predglmLR1,col=3,lty=2)

summary(glmLR1)


#check model fit
par(mfrow=c(2,2),mar=c(4,4,2,0.2))
plot(glmLR1)

```

## Titanic example

here's an interesting logistic regression example, one can tell a good story around it

Vamos ler os dados do package `titanic`.

```{r}
library(titanic)
```

Vamos ver os dados

```{r}
head(titanic_train)
```

### Análise Exploratoria de Dados

```{r}
barplot(table(titanic_train$Survived))
```


```{r}
with(titanic_train,plot(Survived~Age))
```


```{r}
with(titanic_train,boxplot(Survived~Sex))
```



```{r}
with(titanic_train,boxplot(Age~Sex))
```

### Modelação

Testar um modelo linear

```{r}
lm1<-lm(Survived~Age,data=titanic_train)
with(titanic_train,plot(Survived~Age))
abline(lm1)
```

tentar com um GLM

```{r}
glm1<-glm(Survived~Age,data=titanic_train,family=binomial(link="logit"))
with(titanic_train,plot(Survived~Age))
abline(lm1)
# isto não funciona
#abline(glm1,col="blue")

#primeiro faço a predição
newages <- seq(0,90,length=300)
preds <- predict(glm1,newdata=data.frame(Age=newages),type="response")
#e agora adiciono ao plot
lines(newages,preds,col="green")
```

o que diz o meu modelo

```{r}
summary(glm1)
```

ok, idade influencia, negativamente, a sobrevivencia, e o sexo?

```{r}
glm2<-glm(Survived~Age+Sex,data=titanic_train,family=binomial(link="logit"))
with(titanic_train,plot(Survived~Age))
abline(lm1)
# isto não funciona
#abline(glm1,col="blue")

#primeiro faço a predição
newages <- seq(0,90,length=300)
predsM <- predict(glm2,newdata=data.frame(Age=newages,Sex="male"),type="response")
predsF <- predict(glm2,newdata=data.frame(Age=newages,Sex="female"),type="response")
#e agora adiciono ao plot
lines(newages,preds,col="green")
lines(newages,predsM,col="blue")
lines(newages,predsF,col="pink")
```

ver o model

```{r}
summary(glm2)
```

e a classe3 do bilhete?

```{r}
glm3<-glm(Survived~Age+Sex+Pclass,data=titanic_train,family=binomial(link="logit"))
with(titanic_train,plot(Survived~Age))
abline(lm1)
# isto não funciona
#abline(glm1,col="blue")

#primeiro faço a predição
newages <- seq(0,90,length=300)
predsMC1 <- predict(glm3,newdata=data.frame(Age=newages,Sex="male",Pclass=1),type="response")
predsMC2 <- predict(glm3,newdata=data.frame(Age=newages,Sex="male",Pclass=2),type="response")
predsMC3 <- predict(glm3,newdata=data.frame(Age=newages,Sex="male",Pclass=3),type="response")
predsFC1 <- predict(glm3,newdata=data.frame(Age=newages,Sex="female",Pclass=1),type="response")
predsFC2 <- predict(glm3,newdata=data.frame(Age=newages,Sex="female",Pclass=2),type="response")
predsFC3 <- predict(glm3,newdata=data.frame(Age=newages,Sex="female",Pclass=3),type="response")
#e agora adiciono ao plot
lines(newages,preds,col="green")
lines(newages,predsMC1,col="blue",lwd=1)
lines(newages,predsFC1,col="pink",lwd=1)
lines(newages,predsMC2,col="blue",lwd=2)
lines(newages,predsFC2,col="pink",lwd=2)
lines(newages,predsMC3,col="blue",lwd=3)
lines(newages,predsFC3,col="pink",lwd=3)
```

o modelo

```{r}
summary(glm3)
```


## About multicollinearity

(this is probably best way before, in the linear model stuff... any way, I presented it here as this topic started from a student question)

```{r}
#Código da aula 11

#--------------------------------------------------------
# When good models go wrong as 
# multicollinearity kicks in
#--------------------------------------------------------
library(MASS)
set.seed(1234)
n=100
means <- c(2,4,6,8,10,12)
ncovs=(36-6)/2
covs<- rnorm(ncovs,mean=10,sd=2)
varcovars=matrix(NA,6,6)
varcovars[lower.tri(varcovars)]=covs
varcovars=t(varcovars)
varcovars[lower.tri(varcovars)]=covs
diag(varcovars)=means
varcovars=t(varcovars) %*% varcovars
indvars <- mvrnorm(n = n, mu=means, Sigma=varcovars)

# we can see that we have high correlations 
#across the board in explanatory variables
indvars=as.data.frame(indvars)
names(indvars)=paste0("X",1:6)
head(indvars)
round(cor(indvars),2)

ys <-510+4*indvars$X1+rnorm(n,mean=0,sd=200)
par(mfrow=c(1,1),mar=c(4,4,0.5,0.5))
plot(ys~indvars$X1)
lmX1 <- lm(ys~indvars$X1)
abline(lmX1)
summary(lmX1)

plot(ys~indvars$X2)
lmX2 <- lm(ys~indvars$X2)
abline(lmX2)
summary(lmX2)

#one error type I + 1 error type 2
lmX1X2 <- lm(ys~indvars$X1+indvars$X2)
summary(lmX1X2)


lmX3 <- lm(ys~indvars$X3)
summary(lmX3)
lmX1X3 <- lm(ys~indvars$X1+indvars$X3)
summary(lmX1X3)

lmX4 <- lm(ys~indvars$X4)
summary(lmX4)
lmX1X4 <- lm(ys~indvars$X1+indvars$X4)
summary(lmX1X4)

lmX5 <- lm(ys~indvars$X5)
summary(lmX5)
lmX1X5 <- lm(ys~indvars$X1+indvars$X5)
summary(lmX1X5)


lmX6 <- lm(ys~indvars$X6)
summary(lmX6)
lmX1X6 <- lm(ys~indvars$X1+indvars$X6)
summary(lmX1X6)

AIC(lmX1,lmX2,lmX3,lmX4,lmX5,lmX6,lmX1X2,lmX1X3,lmX1X4,lmX1X5,lmX1X6)

```

The fundamental idea is that most of these models are equally good for prediction. But they might give you the complete wrong picture for explanation. Never confuse the two. Optimal modelling strategies and choices might require know what is more important.
