[["handson.html", "Chapter 7 Hands On Regression 7.1 The assumptions are on the residuals, not the data 7.2 Conditional versus marginal distribution", " Chapter 7 Hands On Regression Here we come up with a story… an example about regression. I would love to weave in an example that could also be used to illustrate Simpsons paradox, as in https://twitter.com/TivadarDanka/status/1646101110065856512 A simple regression is just the situation where we want to model a response variable as a function of a single explanatory variable. As an examples, say, the time a fish takes to react to a predator introduced in an aquarium by getting into shelter, as a function of the water temperature. Let’s simulate some data that would represent this scenario, but I am not showing you the way the data was simulated just yet. Nonetheless, let me tell you that the reaction times were created in object react, the temperatures in object temp, and these were then packed as a data.frame called reaction. The first few lines of the simulated data are shown in Table 7.1. knitr::kable( head(reaction, 5), caption = &#39;The simulated dataset&#39;, booktabs = TRUE ) Table 7.1: The simulated dataset react temp 3.864305 11.84882 6.002570 17.02374 5.400667 15.73326 4.462596 11.68052 6.689445 19.43839 The data is shown in figure 7.1. par(mar = c(4, 4, .1, .1)) plot(react~temp,xlab=&quot;Temperature (degrees, Celcius)&quot;,ylab=&quot;Reaction time (seconds)&quot;) Figure 7.1: An example regression data set that could be explained by a linear regression It seems like there is a linear relationship between the predictor (temperature) and the response (the reaction time). We could therepore model it with a simple linear regression. We can do that using R´s function lm. We do so here and then look at the summary of the object produced. The required argument for lm is the formula that defines the regression model. The symbol ~is used to represent “as a function of”. So here we will want something like “reaction time ~ water temperature”. While this might seem like a detail, it is a good policy to always fit models using explicitly the data argument, instead of fitting the model to objects hanging around the workspace. Learn how to be tidy! Therefore, while the imideate result would be the same, we suggest that you do not do this mod0&lt;-lm(react~temp) nor this mod0&lt;-lm(reaction$react~reaction$temp) but always consider this mod0&lt;-lm(react~temp,data=reaction) summary(mod0) ## ## Call: ## lm(formula = react ~ temp, data = reaction) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43172 -0.13223 -0.01381 0.13843 0.49265 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.55487 0.15642 3.547 0.000881 *** ## temp 0.31442 0.01015 30.986 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2182 on 48 degrees of freedom ## Multiple R-squared: 0.9524, Adjusted R-squared: 0.9514 ## F-statistic: 960.2 on 1 and 48 DF, p-value: &lt; 2.2e-16 This will be easier to read for others, makes you tidy, and will save you headaches when using functions like predict over the resulting fitted model. We can the estimated regression line to the above plot. I color it red to remind us of the fact that this is an estimated line, not the true line that generated the data. While in general we do not know this with real data, here I know the model that was used to simulate the data. Just for comparison I can add it to the plot to compare with the estimated regression line. plot(react~temp,xlab=&quot;Temperature (degrees, Celcius)&quot;,ylab=&quot;Reaction time (seconds)&quot;) abline(mod0,lty=2,col=&quot;red&quot;) abline(beta0,beta1,lty=2,col=&quot;green&quot;) legend(&quot;topleft&quot;,legend=c(&quot;Real model&quot;,&quot;Estimated Regression Line&quot;),col=c(&quot;green&quot;,&quot;red&quot;),lty=2, inset=0.05) The estimated line and the true line are very similar, as expected since we have a reasonable sample size, a small error, and a model that is the reality. With real data, this will be the exception, not the rule. All models are wrong, but some are useful. The linear regression model is perhaps one of the simplest, but also one of the most widely used, and hence, one of those that has been extremely useful. But of course, its simplicity is also its major disadvantage, as we shall see. 7.1 The assumptions are on the residuals, not the data Imagine that you have a single variable that you are interested in modelling. This is the concentration of an enzyme in the blood of small rodents, from 4 different species. The data is created and saved as file enzimes.txt, but I do not show here how it is generated for dramatic effects! This is represented in the image below 7.2. hist(ys,breaks=0:40,xlab=&quot;Concentration of enzime (mg/L)&quot;) Figure 7.2: Concentration of an enzime (mg/L) in the blood of small rodents, from 4 diffferent species A poor (conventional and traditional) biologist would die if shown this dataset - it looks nothing like Gaussian, what shall he do? - but the truth is there would be no reason for it. If one accounts for the different species, this is what we see. Clear differences between two groups of species. boxplot(ys~type,ylab=&quot;Concentration of enzime (mg/L)&quot;) And further, we can see that the remaining residuals are a beautiful Gaussian. Not a surprise, since this was simulated data, from a Gaussian model :) ! hist(residuals(lm(ys~type)),main=&quot;&quot;,xlab=&quot;Residuals&quot;) The take home message from the story: what the data looks like might be irrelevant. The patterns that remain in the residuals, if any, those are the ones we might need to worry about. So do not transform data just because the data looks odd. It might just be Gaussian data in disguise! 7.2 Conditional versus marginal distribution (I have a vague recollection I have written this somewhere else in this document before, but can’t find it - if this is redundant might need to consolidate material later) One fundamental aspect is that while the data, in other words, the distribution of the response, say \\(Y\\), does not need to be Gaussian, the linear model assumption on the residuals implies that the distribution of the response, conditional on the value of the covariate(s), will be Gaussian. In other words, if we have a linear model, then \\[ y_i=\\beta_0+\\beta_1 X_i+e_i\\] where the \\(e_i\\) are Gaussian with mean 0 and variance \\(\\sigma^2_i\\), then it follows that conditioning on the covariate values, i.e. given the covariate values, we know the distribution of \\(Y\\), given by \\[Y|X=Gaussian(\\beta_0+\\beta_1 X,\\sigma^2)\\] This highlights a different way to simulate data for a Gaussian regression and you can check this leads to the exact same thing par(mar = c(4, 4, .1, .1)) plot(react~temp,xlab=&quot;Temperature (degrees, Celcius)&quot;,ylab=&quot;Reaction time (seconds)&quot;) Figure 7.3: An example regression data set that could be explained by a linear regression (same as above!) "]]
