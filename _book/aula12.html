<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Class 13: 10 11 2020 Maximum likelihood and all that | Notes for Ecological Modelling</title>
  <meta name="description" content="This is based on Yihui Xie’s a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Class 13: 10 11 2020 Maximum likelihood and all that | Notes for Ecological Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is based on Yihui Xie’s a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Class 13: 10 11 2020 Maximum likelihood and all that | Notes for Ecological Modelling" />
  
  <meta name="twitter:description" content="This is based on Yihui Xie’s a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Tiago A. Marques" />


<meta name="date" content="2020-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conlusion-on-linear-regression.html"/>
<link rel="next" href="aula14.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="mainintro.html"><a href="mainintro.html"><i class="fa fa-check"></i><b>2</b> Preamble</a></li>
<li class="chapter" data-level="3" data-path="aknowledgments.html"><a href="aknowledgments.html"><i class="fa fa-check"></i><b>3</b> Aknowledgments</a></li>
<li class="chapter" data-level="4" data-path="usingRintro.html"><a href="usingRintro.html"><i class="fa fa-check"></i><b>4</b> Using R</a></li>
<li class="chapter" data-level="5" data-path="about-regression.html"><a href="about-regression.html"><i class="fa fa-check"></i><b>5</b> About regression</a><ul>
<li class="chapter" data-level="5.1" data-path="about-regression.html"><a href="about-regression.html#what-is-a-regression"><i class="fa fa-check"></i><b>5.1</b> What is a regression?</a></li>
<li class="chapter" data-level="5.2" data-path="about-regression.html"><a href="about-regression.html#the-general-linear-model"><i class="fa fa-check"></i><b>5.2</b> The general linear model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="handson.html"><a href="handson.html"><i class="fa fa-check"></i><b>6</b> Hands On Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="handson.html"><a href="handson.html#the-assumptions-are-on-the-residuals-not-the-data"><i class="fa fa-check"></i><b>6.1</b> The assumptions are on the residuals, not the data</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="aula6.html"><a href="aula6.html"><i class="fa fa-check"></i><b>7</b> Class 6 13 10 2020</a><ul>
<li class="chapter" data-level="7.1" data-path="aula6.html"><a href="aula6.html#implementing-a-regression"><i class="fa fa-check"></i><b>7.1</b> Implementing a regression</a></li>
<li class="chapter" data-level="7.2" data-path="aula6.html"><a href="aula6.html#simulating-regression-data"><i class="fa fa-check"></i><b>7.2</b> Simulating regression data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="aula6.html"><a href="aula6.html#what-is-the-effect-of-increasing-the-error-a-simulation-experiment"><i class="fa fa-check"></i><b>7.2.1</b> What is the effect of increasing the error: a simulation experiment</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aula7.html"><a href="aula7.html"><i class="fa fa-check"></i><b>8</b> Class 7 14 10 2020</a><ul>
<li class="chapter" data-level="8.1" data-path="aula7.html"><a href="aula7.html#task-1"><i class="fa fa-check"></i><b>8.1</b> Task 1</a></li>
<li class="chapter" data-level="8.2" data-path="aula7.html"><a href="aula7.html#task-2"><i class="fa fa-check"></i><b>8.2</b> Task 2</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="aula8.html"><a href="aula8.html"><i class="fa fa-check"></i><b>9</b> Class 8 20 10 2020 - t-test and ANOVA are just linear models</a><ul>
<li class="chapter" data-level="9.1" data-path="aula8.html"><a href="aula8.html#the-t-test"><i class="fa fa-check"></i><b>9.1</b> The t-test</a></li>
<li class="chapter" data-level="9.2" data-path="aula8.html"><a href="aula8.html#anova"><i class="fa fa-check"></i><b>9.2</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="aula9.html"><a href="aula9.html"><i class="fa fa-check"></i><b>10</b> Class 9: 21 10 2020 - ANCOVA is (also) just a linear model</a><ul>
<li class="chapter" data-level="10.1" data-path="aula9.html"><a href="aula9.html#common-slope-different-intercepts-per-treatment"><i class="fa fa-check"></i><b>10.1</b> Common slope, different intercepts per treatment</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="aula10.html"><a href="aula10.html"><i class="fa fa-check"></i><b>11</b> Class 10: 27 10 2020</a><ul>
<li class="chapter" data-level="11.1" data-path="aula10.html"><a href="aula10.html#same-story-another-spin"><i class="fa fa-check"></i><b>11.1</b> Same story, another spin</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="aula11.html"><a href="aula11.html"><i class="fa fa-check"></i><b>12</b> Class 11: 03 11 2020 ANCOVA with different slopes: interactions</a><ul>
<li class="chapter" data-level="12.1" data-path="aula11.html"><a href="aula11.html#about-interactions"><i class="fa fa-check"></i><b>12.1</b> About interactions</a></li>
<li class="chapter" data-level="12.2" data-path="aula11.html"><a href="aula11.html#task-1-implementing-the-ancova-with-different-slopes"><i class="fa fa-check"></i><b>12.2</b> Task 1 Implementing the ANCOVA with different slopes</a></li>
<li class="chapter" data-level="12.3" data-path="aula11.html"><a href="aula11.html#task-2-modeling-a-data-set"><i class="fa fa-check"></i><b>12.3</b> Task 2 Modeling a data set</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="aula13.html"><a href="aula13.html"><i class="fa fa-check"></i><b>13</b> Class 12: 04 11 2020 Interactions between continous covariates</a><ul>
<li class="chapter" data-level="13.1" data-path="aula13.html"><a href="aula13.html#larger-order-interactions"><i class="fa fa-check"></i><b>13.1</b> Larger order interactions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="conlusion-on-linear-regression.html"><a href="conlusion-on-linear-regression.html"><i class="fa fa-check"></i><b>14</b> Conlusion on linear regression</a><ul>
<li class="chapter" data-level="14.1" data-path="conlusion-on-linear-regression.html"><a href="conlusion-on-linear-regression.html#conclusion"><i class="fa fa-check"></i><b>14.1</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="aula12.html"><a href="aula12.html"><i class="fa fa-check"></i><b>15</b> Class 13: 10 11 2020 Maximum likelihood and all that</a><ul>
<li class="chapter" data-level="15.1" data-path="aula12.html"><a href="aula12.html#maximizing-a-likelihood-algebraically"><i class="fa fa-check"></i><b>15.1</b> Maximizing a likelihood algebraically</a></li>
<li class="chapter" data-level="15.2" data-path="aula12.html"><a href="aula12.html#numerically-maximizing-a-likelihood"><i class="fa fa-check"></i><b>15.2</b> Numerically Maximizing a likelihood</a></li>
<li class="chapter" data-level="15.3" data-path="aula12.html"><a href="aula12.html#the-case-of-a-gaussian"><i class="fa fa-check"></i><b>15.3</b> The case of a Gaussian</a></li>
<li class="chapter" data-level="15.4" data-path="aula12.html"><a href="aula12.html#the-case-of-a-linear-model"><i class="fa fa-check"></i><b>15.4</b> The case of a linear model</a></li>
<li class="chapter" data-level="15.5" data-path="aula12.html"><a href="aula12.html#the-really-interesting-case"><i class="fa fa-check"></i><b>15.5</b> The really interesting case</a></li>
<li class="chapter" data-level="15.6" data-path="aula12.html"><a href="aula12.html#likelihood-above-and-beyond"><i class="fa fa-check"></i><b>15.6</b> Likelihood, above and beyond</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="aula14.html"><a href="aula14.html"><i class="fa fa-check"></i><b>16</b> Class 14: 10 10 2020 GLMs</a><ul>
<li class="chapter" data-level="16.1" data-path="aula14.html"><a href="aula14.html#what-are-glms"><i class="fa fa-check"></i><b>16.1</b> What are GLMs</a></li>
<li class="chapter" data-level="16.2" data-path="aula14.html"><a href="aula14.html#the-link-function"><i class="fa fa-check"></i><b>16.2</b> The link function</a></li>
<li class="chapter" data-level="16.3" data-path="aula14.html"><a href="aula14.html#most-useful-glm-families"><i class="fa fa-check"></i><b>16.3</b> Most useful GLM Families</a></li>
<li class="chapter" data-level="16.4" data-path="aula14.html"><a href="aula14.html#an-example-analysis"><i class="fa fa-check"></i><b>16.4</b> An example analysis</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="aula15.html"><a href="aula15.html"><i class="fa fa-check"></i><b>17</b> Class 15: 11 11 2020</a></li>
<li class="chapter" data-level="18" data-path="aula16.html"><a href="aula16.html"><i class="fa fa-check"></i><b>18</b> Class 16: 17 11 2020</a></li>
<li class="chapter" data-level="19" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>19</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for Ecological Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aula12" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> Class 13: 10 11 2020 Maximum likelihood and all that</h1>
<p>We have been fitting regression models, using say function <code>lm</code>. While this might seem rather ordinary and uninteresting to a seasoned statistician, it is quite remarkable for the average person. So remarkable that I would suggest the analogy of a cell phone to most of us. We do not really think about it, since we do it all the time, but wait a second: can you imagine all the things that must happen inside that little device so that your cousin Maria João having her honey moon in Hawaii can share with you a 2-second delay live of the fantastic romantic dinner she is having, while you are actually 10000 meters above ground on a plane preparing to land in Siberia? For about 99.9 % of you, you do not! And I do not plan on telling you here - I hope by now you have realized that is beyond the purpose of this book. On the contrary, telling you exactly what happens behind the scenes when a function like <code>lm</code> reports some maximum likelihood estimates of a given model parameters is the task that lays ahead. “Brace brace!”, as they might say when facing strong turbulence on your plane that currently is landing in Siberia.</p>
<p>What <code>lm</code> does under the hood is, based on a model, estimate the best value of the parameters, given the data. We illustrate it using the standard linear model, with a single covariate <span class="math inline">\(x\)</span> to explain the response <span class="math inline">\(y\)</span></p>
<p><span class="math display">\[y=\alpha+\beta x+e_i\]</span></p>
<p>where <span class="math inline">\(e_i~Gau(0,\sigma^2)\)</span>. The <code>lm</code> function finds the best values of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span>, given the data. Those we call estimators, and denote them by <span class="math inline">\(\hat \alpha\)</span>, <span class="math inline">\(\hat \beta\)</span> and <span class="math inline">\(\hat \sigma\)</span>. After collecting a sample, we fit the model and we get the estimates. Remember estimates are observed values, or realizations based on the sample, of estimators.</p>
<p>The way <code>lm</code> finds the estimates is via maximum likelihood. Actually, this happens despite the fact that the line is widely know as the minimum squares line. Why is that? Because as we saw in chapter ?, that line is the line that minimizes the sum of the squares of the deviations between the observations and the predictions conditional on the best line. Formally, that is the line that minimizes the following quantity</p>
<p><span class="math display">\[\sum_{i=1}^n (y_i-\hat y_i)^2=\sum_{i=1}^n (y_i-(\hat \alpha+ \hat \beta x_i))^2.\]</span></p>
<p>But what is the likelihood and does it work? We will introduce the concept using an example. Imagine a biologist, lets call her Andreia. Andreia is interested in estimating the probability that a pair of jays will produce eggs before the first of June.</p>
<p>She sets out to find a random sample of blue jay nests, and defines a random variable <span class="math inline">\(X\)</span> representing the egg status of a nest on the 1<span class="math inline">\(^{st}\)</span> of June. We assume that all eggs laid before 1 June will not have fledged yet). Andreia decides that <span class="math inline">\(X\)</span> will take:</p>
<ul>
<li>the value 1 if eggs are present, which she will call a success, and she assumes that happens with probability <span class="math inline">\(\theta\)</span>,</li>
<li>the value 0, which she calls a failure, representing no eggs present, with probability 1-<span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Assuming that the probability of different nests having eggs is independent, each of these is a Bernoulli trial, and there are <span class="math inline">\(N\)</span> trials, of which we could say <span class="math inline">\(n\)</span> will be successes, and <span class="math inline">\(n-n\)</span> will be failures. The Bernoulli distribution is a special case of a Binomial random variable, with a single trial and probability of success <span class="math inline">\(\theta\)</span>. In fact, you you consider all the nests together that is indeed a Binomial with parameters <span class="math inline">\(N\)</span> and <span class="math inline">\(p\)</span>. A small detour to justify this statement: there is a theoretical result that demonstrates that the sum of <span class="math inline">\(K\)</span> independent Binomials <span class="math inline">\(X_k\)</span>, each with <span class="math inline">\(N_k\)</span> trials, with constant probability of success <span class="math inline">\(p\)</span>, is a Binomial(<span class="math inline">\(N,p\)</span>), where <span class="math inline">\(N=\sum_{k=1}^K N_k\)</span>. Therefore, the sum of <span class="math inline">\(k\)</span> Bernoulli trials, i.e. <span class="math inline">\(k\)</span> Binomial(1,p) independent random variables, is a Binomial(<span class="math inline">\(K,p\)</span>).</p>
<p>So this is a model with a single parameter, <span class="math inline">\(\theta\)</span>. (since we know <span class="math inline">\(N\)</span>, the number of trials!)</p>
<p>Andreia goes out and about in the field and finds 5 nests. The first has eggs, the second and third do not, the forth does, and the fifth does not. By this time Andreia is tired and decides to call it a day, with her sample <span class="math inline">\(x\)</span> collected: <span class="math inline">\(x=c(1,0,0,1,0)\)</span>.</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" title="1"><span class="co">#a 1 is a nest with eggs, a 0 is a nest without eggs</span></a>
<a class="sourceLine" id="cb291-2" title="2">nests=<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a></code></pre></div>
<p>Note that in this case the number of successes n is 2 and the number of failures <code>N-n</code> is 3.</p>
<p>Then she asks a friend doing an MSc in biostatistics how she can estimate the value of <span class="math inline">\(\theta\)</span>. Unfortunately, her friend has just started her classes, and she too is a bit unsure about what to do too. But she does know how to calculate the probability of the observed sample.</p>
<p><span class="math display">\[P(x|\theta)=\theta (1-\theta) (1-\theta) \theta (1-\theta)=\theta^2 (1-\theta)^3\]</span></p>
<p>If only we knew what the value of was we could evaluate this probability. Imagine that it was 0.2, then the probability of the sample would be <span class="math inline">\(0.2^2 0.8^3\)</span>=0.02048. What if it was 0.8, then the probability of the data would be <span class="math inline">\(0.8^2 0.2^3\)</span>=0.00512. This is a considerably lower probability.</p>
<p>And here’s when Andreia’s friend has a great idea. What if we turn it around and look at this as a function of theta, conditional on the data</p>
<p><span class="math display">\[P(\theta|x)=\theta (1-\theta) (1-\theta) \theta (1-\theta)=\theta^2 (1-\theta)^3\]</span></p>
<p>Then we could evaluate the expression for a set of possible values for <span class="math inline">\(\theta\)</span>, and the largest probability will intuitively correspond to the most likely value of <span class="math inline">\(\theta\)</span>.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" title="1"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb292-2" title="2">thetas&lt;-<span class="kw">seq</span>(<span class="fl">0.05</span>,<span class="fl">0.95</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb292-3" title="3">pthetas&lt;-thetas<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">^</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb292-4" title="4"><span class="kw">kable</span>(<span class="kw">cbind</span>(thetas,pthetas),<span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>,<span class="st">&quot;P(theta)&quot;</span>))</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">theta</th>
<th align="right">P(theta)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.05</td>
<td align="right">0.0021434</td>
</tr>
<tr class="even">
<td align="right">0.15</td>
<td align="right">0.0138178</td>
</tr>
<tr class="odd">
<td align="right">0.25</td>
<td align="right">0.0263672</td>
</tr>
<tr class="even">
<td align="right">0.35</td>
<td align="right">0.0336416</td>
</tr>
<tr class="odd">
<td align="right">0.45</td>
<td align="right">0.0336909</td>
</tr>
<tr class="even">
<td align="right">0.55</td>
<td align="right">0.0275653</td>
</tr>
<tr class="odd">
<td align="right">0.65</td>
<td align="right">0.0181147</td>
</tr>
<tr class="even">
<td align="right">0.75</td>
<td align="right">0.0087891</td>
</tr>
<tr class="odd">
<td align="right">0.85</td>
<td align="right">0.0024384</td>
</tr>
<tr class="even">
<td align="right">0.95</td>
<td align="right">0.0001128</td>
</tr>
</tbody>
</table>
<p>The largest values are observed for the trial values of <span class="math inline">\(\theta\)</span> of 0.35 and 0.45. What if we calculate that probability for a fine grid of values possible for <span class="math inline">\(\theta\)</span> and represent it in a plot. This is what follows, and we add to the plot a dashed vertical line representing the value of <span class="math inline">\(\theta\)</span> for which that function is maximized.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" title="1">thetas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb293-2" title="2">pthetas&lt;-thetas<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">^</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb293-3" title="3"><span class="kw">plot</span>(thetas,pthetas,<span class="dt">ylab=</span><span class="st">&quot;P(</span><span class="ch">\t</span><span class="st">heta|x)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;theta&quot;</span>)</a>
<a class="sourceLine" id="cb293-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span>thetas[pthetas<span class="op">==</span><span class="kw">max</span>(pthetas)],<span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.3-1.png" width="672" /></p>
<p>You will probably not be too surprised to find out that <span class="math inline">\(\hat \theta\)</span>=0.4 is indeed the maximum likelihood estimate (MLEe), and that for a proportion <span class="math inline">\(\hat \theta\)</span>=successes/trials is the maximum likelihood estimate (MLE). Therefore, note that it will only be from the context that one can say if MLE stands for an estimate, which corresponds to a random realization of the estimator, or for the estimator itself. This should not have come as a surprise. Remember we had 2 successes in 5 trials, and that corresponds to 0.4.</p>
<p>Andreia asks her friend what was the point. If the MLE was just the empirical proportion, 2/5=0.4, why going through all this trouble? There’s at least 3 good reasons for that:</p>
<ul>
<li>This way we understand why an MLE is</li>
<li>If you look at the figure above, we not only have an estimate of the parameter <span class="math inline">\(\theta\)</span>, but we also have an idea about the precision around that estimate. That comes from the shape of the likelihood profile. We get back to this below.</li>
<li>by embedding it in the concept of a likelihood, we open the door to generalize this procedure to any other far more complicated situation for which closed form analytic estimators do not exist. As examples of additional sophistication, we could easily:
<ul>
<li>consider several parameters at once; as an example, we could be considering instead of a Bernoulli a complex model that describes how a whale dives, with 17 parameters that we want to maximize at once. Rarely closed form estimators will be available then;</li>
<li>make the parameters a function of observed covariates. In the case of our nests, the height of the nest could be a relevant covariate to model the probability of success of a nest, say. In such a case, we could have an estimate for <span class="math inline">\(\theta\)</span> that would be dependent of the height <code>h</code>, e.g by defining that <span class="math inline">\(\theta_h=f(h)\)</span>. Naturally we would choose the link function f such that theta would be constrained to be between 0 and 1, the possible values for a probability. The logit link function comes to mind here. But that will be a story for another day.</li>
</ul></li>
</ul>
<p>To illustrate the point above regarding being able to estimate the precision around the parameter estimate from the likelihood function, lets consider that we had not 5 samples, but man more. In the figure below we contrast the small sample size to a set of increasing sample sizes: 50, 100 or 200.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb294-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">1</span>),<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb294-2" title="2">thetas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb294-3" title="3">pthetas&lt;-thetas<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">^</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb294-4" title="4"><span class="kw">plot</span>(thetas,pthetas,<span class="dt">ylab=</span><span class="st">&quot;P(</span><span class="ch">\t</span><span class="st">heta|x)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;theta (n=5)&quot;</span>)</a>
<a class="sourceLine" id="cb294-5" title="5"><span class="kw">abline</span>(<span class="dt">v=</span>thetas[pthetas<span class="op">==</span><span class="kw">max</span>(pthetas)],<span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb294-6" title="6">thetas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb294-7" title="7">pthetas&lt;-thetas<span class="op">^</span><span class="dv">20</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">^</span><span class="dv">30</span></a>
<a class="sourceLine" id="cb294-8" title="8"><span class="kw">plot</span>(thetas,pthetas,<span class="dt">ylab=</span><span class="st">&quot;P(</span><span class="ch">\t</span><span class="st">heta|x)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;theta  (n=50)&quot;</span>)</a>
<a class="sourceLine" id="cb294-9" title="9"><span class="kw">abline</span>(<span class="dt">v=</span>thetas[pthetas<span class="op">==</span><span class="kw">max</span>(pthetas)],<span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb294-10" title="10">thetas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb294-11" title="11">pthetas&lt;-thetas<span class="op">^</span><span class="dv">40</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">^</span><span class="dv">60</span></a>
<a class="sourceLine" id="cb294-12" title="12"><span class="kw">plot</span>(thetas,pthetas,<span class="dt">ylab=</span><span class="st">&quot;P(</span><span class="ch">\t</span><span class="st">heta|x)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;theta  (n=100)&quot;</span>)</a>
<a class="sourceLine" id="cb294-13" title="13"><span class="kw">abline</span>(<span class="dt">v=</span>thetas[pthetas<span class="op">==</span><span class="kw">max</span>(pthetas)],<span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb294-14" title="14">thetas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb294-15" title="15">pthetas&lt;-thetas<span class="op">^</span><span class="dv">80</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">^</span><span class="dv">120</span></a>
<a class="sourceLine" id="cb294-16" title="16"><span class="kw">plot</span>(thetas,pthetas,<span class="dt">ylab=</span><span class="st">&quot;P(</span><span class="ch">\t</span><span class="st">heta|x)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;theta  (n=200)&quot;</span>)</a>
<a class="sourceLine" id="cb294-17" title="17"><span class="kw">abline</span>(<span class="dt">v=</span>thetas[pthetas<span class="op">==</span><span class="kw">max</span>(pthetas)],<span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.4-1.png" width="672" /></p>
<p>As we increase the sample size, and hence we increase the amount of information available to estimate <span class="math inline">\(\theta\)</span>, the likelihood profile becomes more spiky. It can be demonstrated that the curvature of the likelihood profile allows us to quantify the precision on our estimate of the parameter. Naturally, the steeper the curve, the better, in the sense that the more certain we are. On the other hand, when the likelihood surface is very flat, we might hit problems in terms of the numerical maximization of the likelihood.</p>
<div id="maximizing-a-likelihood-algebraically" class="section level2">
<h2><span class="header-section-number">15.1</span> Maximizing a likelihood algebraically</h2>
<p>While above we were able to maximize the likelihood function via a “grid” search. We divided the possible range of values that the parameter could take, also know as the parameter space, into a large number of candidate values. Then we evaluated the likelihood at each one of these, and picked the value of the parameter for which the function was maximum: the maximum likelihood estimate.</p>
<p>Grid search can become very inefficient very fast, and hence there are other ways to maximize a likelihood. One is to analytically find what is the maximum of that function. How can we do that. Straightforwardly for our example. You differentiate the function, find the point at which the first derivative is 0, and by definition that point is a maximum or a minimum. If you are unsure the second derivative would tell you which. Considering the above</p>
<p><span class="math display">\[\frac{d(f(\theta))}{d \theta}=\frac{d(\theta^n (1-\theta)^{N-n})}{d \theta}\]</span></p>
<p>Then by solving</p>
<p><span class="math display">\[\frac{d(f(\theta))}{d \theta}=0\]</span></p>
<p>we get that <span class="math inline">\(\hat \theta = n/N\)</span>, which is just the empirical proportion (i.e. the observed proportion of successes in the sample).</p>
<p>(note to self: add detail to these derivations above)</p>
<p>However, like the grid search, this is not a problem free procedure. The above expression was simple enough that derivation was trivial. That might not be the rule, but the exception, so we need an alternative approach for when models are more complex than our Bernoulli example. That will be the norm in real ecological models.</p>
</div>
<div id="numerically-maximizing-a-likelihood" class="section level2">
<h2><span class="header-section-number">15.2</span> Numerically Maximizing a likelihood</h2>
<p>Here we look at using a numerical maximization procedure, which means that we will derive a procedure, and algorithm, that will find the maximum of a function computationally. The analogy with the real world is simple. Imagine that you were somewhere in the most boring country in the world, Boredomnesia. It happens to be a square with a single mountain at the center, as depicted in the image below, and you wanted to start walking and reaching the highest point in the country. Boredomnesia happens to also be the foggiest country in the world, so you manage to see about 3 meters around you, at most!</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" title="1"><span class="co"># need mvtnorm package</span></a>
<a class="sourceLine" id="cb295-2" title="2"><span class="kw">library</span>(<span class="st">&quot;mvtnorm&quot;</span>)</a>
<a class="sourceLine" id="cb295-3" title="3">range =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb295-4" title="4">mean =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb295-5" title="5">Sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">.5</span>, <span class="fl">.5</span>, <span class="dv">1</span>), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb295-6" title="6">out =<span class="st"> </span><span class="kw">matrix</span> (<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">101</span><span class="op">*</span><span class="dv">101</span>),<span class="dv">101</span>)</a>
<a class="sourceLine" id="cb295-7" title="7"></a>
<a class="sourceLine" id="cb295-8" title="8"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(range)){</a>
<a class="sourceLine" id="cb295-9" title="9">	<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(range)){</a>
<a class="sourceLine" id="cb295-10" title="10">		out[i,j] =<span class="st"> </span><span class="kw">dmvnorm</span>(<span class="kw">c</span>(range[i],range[j]),<span class="dt">mean=</span>mean,<span class="dt">sigma=</span>Sigma)</a>
<a class="sourceLine" id="cb295-11" title="11">	}</a>
<a class="sourceLine" id="cb295-12" title="12">}</a>
<a class="sourceLine" id="cb295-13" title="13"> <span class="kw">persp</span>(out,<span class="dt">theta =</span> <span class="dv">30</span>,<span class="dt">phi =</span> <span class="dv">20</span>,<span class="dt">col=</span><span class="st">&quot;lightblue&quot;</span>,<span class="dt">xlab =</span> <span class="st">&quot;Latitude&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Longitude&quot;</span>,<span class="dt">zlab=</span><span class="st">&quot;Elevation&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:country"></span>
<img src="ECOMODbook_files/figure-html/country-1.png" alt="Ilustrating a likelihood. This would be Boredomnesia, the most boring country in the world. You want to to find a way to reach its highest point amidst the dense fog that characterizes it. How can you do it? Just keep moving up!" width="80%" />
<p class="caption">
Figure 15.1: Ilustrating a likelihood. This would be Boredomnesia, the most boring country in the world. You want to to find a way to reach its highest point amidst the dense fog that characterizes it. How can you do it? Just keep moving up!
</p>
</div>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" title="1"><span class="co">#add an emoji that is a small man to it!</span></a></code></pre></div>
<p>But you actually have no idea about the orography of the country, and you can only see so much as the country is always a bit foggy. To make sure you manage, you could set up a set of rules for yourself:</p>
<ol style="list-style-type: decimal">
<li>starting where you are (this seems like a lame first step, but you will see surprisingly that is actually one of the hardest for a computer!). Then, until you can’t find a higher point, repeat the following steps:</li>
<li>evaluate the height where you are currently</li>
<li>evaluate the height at 8 directions around you (like North, South, East, West and the 4 intermediate directions, say)</li>
<li>move toward the steepest highest of those directions</li>
<li>if the difference in elevation (i.e. the mountain slope) is
<ul>
<li>high: move 3 meters</li>
<li>low: move 1 meter</li>
</ul></li>
</ol>
<p>If you do these steps above, when you stop you are at most 1 meter from the top. Well done, you are the king of the world.</p>
<p>Naturally, this assumes the terrain of the country you are in is relatively simple. More precisely, that there is only one mountain in the country, and there are no valleys (or in a likelihood world, no local maxima). Basically, you would not like to be Dane, or Dutch, as there are no mountains there to begin with, and definitely you would not want to be near the Grand Canyon (Figure X) or in Scotland (Figure X), where the Munro’s would certainly defeat you. As we will see below, this has very important implications in the likelihood world!</p>
<div class="figure">
<img src="extfiles/grand-canyon.jpg" alt="The nightmare place for our example task of finding the highest place using the move-towards-higher-ground algorithm, given all the plateaus" />
<p class="caption">The nightmare place for our example task of finding the highest place using the move-towards-higher-ground algorithm, given all the plateaus</p>
</div>
<div class="figure">
<img src="extfiles/munros.jpg" alt="The nightmare place for our example task of finding the highest place using the move-towards-higher-ground algorithm, given all the local maxima" />
<p class="caption">The nightmare place for our example task of finding the highest place using the move-towards-higher-ground algorithm, given all the local maxima</p>
</div>
<p>Now… what happens inside a computer? The above example makes more sense if we are maximizing a likelihood with respect to two parameters, so that the likelihood surface is a bi-dimensional surface. Imagine a Gaussian, for which we want to estimate the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. In the real world the analogy is latitude is equivalent to <span class="math inline">\(\mu\)</span>, longitude is equivalent to <span class="math inline">\(\sigma\)</span>, and the altitude is the likelihood. So now we look at how a computer does it!</p>
<p>There are many algorithms one could use, and here we will use some standard R functions to do the job for us. We will consider a couple, <code>optimize</code>, when we are only considering a single parameter, and <code>optim</code>, for when more than one parameter is at stake. An alternative to <code>optim</code> might be <code>nlm</code> (from package <code>stats</code>). There are many other options in and outside R!</p>
<p>The first thing we need to do is to write up the likelihood function. This will be often the hardest part. That would be like having a detailed map of the country. We know that takes a lot of work to do.</p>
<p>This must be a function which the first argument is the parameter(s), typically the second is the data. Then other additional parameters might follow, or not.</p>
<p>Let us build, step by step, a likelihood for the example of the Bernoulli case for the nests we were looking at in the previous section.</p>
<p>Recall the probability of <span class="math inline">\(\theta\)</span>, given the data 1,0,1,0,0.</p>
<p><span class="math display">\[P(\theta|x)=\theta (1-\theta) (1-\theta) \theta (1-\theta)=\theta^2 (1-\theta)^3\]</span></p>
<p>We can write a bespoke function of theta to evaluate this probability</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" title="1">liktheta1=<span class="cf">function</span>(theta){</a>
<a class="sourceLine" id="cb297-2" title="2">  lik&lt;-theta<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb297-3" title="3">  <span class="kw">return</span>(lik)</a>
<a class="sourceLine" id="cb297-4" title="4">}</a></code></pre></div>
<p>Now we use it, job done</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb298-1" title="1"><span class="kw">liktheta1</span>(<span class="fl">0.35</span>)</a></code></pre></div>
<pre><code>## [1] 0.03364156</code></pre>
<p>just as in the table above, we are good. But this is not really what we want, because the data, our sample was hardwired, we need a function that could cope with any sample. We need to be able to compute the relevant statistics from the sample. Andreia realizes that she can do that easily by summing successes and failures in the table</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb300-1" title="1"><span class="kw">sum</span>(nests<span class="op">==</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb302-1" title="1"><span class="kw">sum</span>(nests<span class="op">==</span><span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>and hence she suggests this new formulation</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb304-1" title="1">liktheta2=<span class="cf">function</span>(theta,samp){</a>
<a class="sourceLine" id="cb304-2" title="2">  lik&lt;-theta<span class="op">^</span><span class="kw">sum</span>(samp<span class="op">==</span><span class="dv">1</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span><span class="kw">sum</span>(samp<span class="op">==</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb304-3" title="3">  <span class="kw">return</span>(lik)</a>
<a class="sourceLine" id="cb304-4" title="4">}</a></code></pre></div>
<p>She tries it out</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" title="1"><span class="kw">liktheta2</span>(<span class="fl">0.35</span>,nests)</a></code></pre></div>
<pre><code>## [1] 0.03364156</code></pre>
<p>and she gets the same value as above. She’s happy, as she can now calculate the likelihood for (1) any parameter value and (2) any sample. Excited, she shows how this would be the case for 11 nests, with just 1 success.</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb307-1" title="1"><span class="kw">liktheta2</span>(<span class="fl">0.35</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span>)))</a></code></pre></div>
<pre><code>## [1] 0.00471196</code></pre>
<p>Now, she’s really excited and she has a dream where she samples 1000 eggs, and 300 successes. She wakes up and wants to know the likelihood under that scenario</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" title="1">dreameggs&lt;-<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">300</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">700</span>))</a>
<a class="sourceLine" id="cb309-2" title="2"><span class="kw">liktheta2</span>(<span class="fl">0.35</span>,dreameggs)</a></code></pre></div>
<pre><code>## [1] 1.818708e-268</code></pre>
<p>Ups, something went terribly wrong, the likelihood is now… 0. This is unhelpful, I can’t climb a mountain if there is no mountain! She scratches her head for a while and she realizes what is going on. She’s multiplying 5000 probabilities, even if those were high, the computer will round them to 0.</p>
<p>Andreia calls a friend, and he says that he will give her two clues that might help. And then says:</p>
<ol style="list-style-type: decimal">
<li><p>If you apply the log to a function, the logged function will have the same function as the untransformed function, and</p></li>
<li><p>log(a*b)=log(a+b)</p></li>
</ol>
<p>Andreia hangs up the phone and takes a mental note: “I need to find better, more useful friends”! But during the night she has an epiphany. If she logs the function, a product of probabilities, she will get a sum of log probabilities. Log probabilities are smaller than probabilities, but there’s a small miracle in the process. The sum of small numbers does not tend (does not converge to 0!). And so she tries a new function, where she adds the log probabilities</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" title="1">logliktheta=<span class="cf">function</span>(theta,data){</a>
<a class="sourceLine" id="cb311-2" title="2">  loglik=<span class="kw">sum</span>(<span class="kw">log</span>(theta<span class="op">^</span><span class="kw">sum</span>(data<span class="op">==</span><span class="dv">1</span>))<span class="op">+</span><span class="kw">sum</span>(<span class="kw">log</span>((<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span><span class="kw">sum</span>(data<span class="op">==</span><span class="dv">0</span>))))</a>
<a class="sourceLine" id="cb311-3" title="3">  <span class="kw">return</span>(loglik)</a>
<a class="sourceLine" id="cb311-4" title="4">}</a></code></pre></div>
<p>She calculates the function that gave her grief above, and the egg dream meets the epiphany</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb312-1" title="1"><span class="kw">logliktheta</span>(<span class="fl">0.35</span>,dreameggs)</a></code></pre></div>
<pre><code>## [1] -616.4947</code></pre>
<p>Then, she just needs to call the <code>optimize</code>, where <code>interval</code> defines the plausible parameter space, and we make sure that <code>maximum</code> is TRUE because by default the function <code>optimize</code> minimizes (That is why we sometimes use a function that is <code>-log(likelihood)</code>, that means the minimum is the point we want!) the function <code>f</code> with respect to its first parameter, given any other arguments provided to <code>f</code>. In this case those other parameters are just the <code>data</code>, the second argument for <code>liktheta</code>. Those you will recognize as our data.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" title="1">MLEtheta&lt;-<span class="kw">optimize</span>(<span class="dt">f=</span>logliktheta,<span class="dt">interval=</span><span class="kw">c</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>),<span class="dt">data=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>),<span class="dt">maximum=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb314-2" title="2">MLEtheta</a></code></pre></div>
<pre><code>## $maximum
## [1] 0.399996
## 
## $objective
## [1] -3.365058</code></pre>
<p>Now we can actually calculate the MLE for <span class="math inline">\(\theta\)</span> in the case of Andreia’s dream sample.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb316-1" title="1">MLEthetadream&lt;-<span class="kw">optimize</span>(<span class="dt">f=</span>logliktheta,<span class="dt">interval=</span><span class="kw">c</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>),<span class="dt">data=</span>dreameggs,<span class="dt">maximum=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb316-2" title="2">MLEthetadream</a></code></pre></div>
<pre><code>## $maximum
## [1] 0.2999964
## 
## $objective
## [1] -610.8643</code></pre>
<p>So the output of <code>optimize</code> has two components, <span class="math inline">\(maximum\)</span> and <span class="math inline">\(objective\)</span>. What are these?
The two components of this object are the MLE of the parameter, in this case 0.399996 and the value of the function at that point for <span class="math inline">\(\theta\)</span>, in this case -3.3650583. This will be the actual value of the likelihood at this point and might be useful later, but for now we ignore it. Note that 0.399996 is just a numeric approximation of the real value, that we know analytically to be 0.4. These are illustrated below:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" title="1"><span class="co">#valores possiveis para thetas</span></a>
<a class="sourceLine" id="cb318-2" title="2">thetas&lt;-<span class="kw">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb318-3" title="3"><span class="co">#object to hold the values of the likelihood</span></a>
<a class="sourceLine" id="cb318-4" title="4">nthetas &lt;-<span class="st"> </span><span class="kw">length</span>(thetas)</a>
<a class="sourceLine" id="cb318-5" title="5">loglikthetas&lt;-<span class="kw">numeric</span>(nthetas)</a>
<a class="sourceLine" id="cb318-6" title="6"><span class="co">#para cada theta</span></a>
<a class="sourceLine" id="cb318-7" title="7"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nthetas){</a>
<a class="sourceLine" id="cb318-8" title="8">  loglikthetas[i] &lt;-<span class="st"> </span><span class="kw">logliktheta</span>(thetas[i],nests)</a>
<a class="sourceLine" id="cb318-9" title="9">}</a>
<a class="sourceLine" id="cb318-10" title="10"><span class="kw">plot</span>(<span class="dt">x=</span>thetas,<span class="dt">y=</span>loglikthetas,<span class="dt">ylab=</span><span class="st">&quot;Log likelihood&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;theta&quot;</span>)</a>
<a class="sourceLine" id="cb318-11" title="11"><span class="kw">abline</span>(<span class="dt">v=</span>MLEtheta<span class="op">$</span>maximum,<span class="dt">h=</span>MLEtheta<span class="op">$</span>objective,<span class="dt">col=</span><span class="st">&quot;green&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.16-1.png" width="672" /></p>
<p>As a task, what would it require to change the above code to calculate the value of theta if we had 78 trials and 43 sucesses? You got it, just need to change the data</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" title="1">MLEtheta&lt;-<span class="kw">optimize</span>(<span class="dt">f=</span>logliktheta,<span class="dt">interval=</span><span class="kw">c</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>),<span class="dt">data=</span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">43</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">78-43</span>)),<span class="dt">maximum=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb319-2" title="2">MLEtheta</a></code></pre></div>
<pre><code>## $maximum
## [1] 0.5512824
## 
## $objective
## [1] -53.6545</code></pre>
<p>So, now we know all about likelihoods, but Andreia wonders. Why all the trouble, if I could just have calculated the exact value of the MLE as the observed proportion? To that we need to continue with Andreia’s explorations.</p>
</div>
<div id="the-case-of-a-gaussian" class="section level2">
<h2><span class="header-section-number">15.3</span> The case of a Gaussian</h2>
<p>Lets now look at situation where Andreia is interested in characterizing how far away from the nearest river, in a straight line, are the nests from water. She assumes that these might be hypothetically described by a Gaussian random variable. That will be the basis for constructing a likelihood. For her 5 nests, those distances in kilometers are</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" title="1">dists&lt;-<span class="kw">c</span>(<span class="fl">0.78</span>,<span class="fl">1.73</span>,<span class="fl">1.32</span>,<span class="fl">0.54</span>,<span class="fl">2.12</span>)</a></code></pre></div>
<p>Then she thinks about what might the likelihood look like for a Gaussian. She knows R can calculate the density of a Gaussian via <code>rnorm</code>, and so she suggests the following minus log likelihood function:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb322-1" title="1">minuslogliknorm=<span class="cf">function</span>(pars,data){</a>
<a class="sourceLine" id="cb322-2" title="2">  media=pars[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb322-3" title="3">  desvio=pars[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb322-4" title="4">  minusloglik=<span class="op">-</span><span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dnorm</span>(data,<span class="dt">mean=</span>media,<span class="dt">sd=</span>desvio)))</a>
<a class="sourceLine" id="cb322-5" title="5">  <span class="kw">return</span>(minusloglik)</a>
<a class="sourceLine" id="cb322-6" title="6">}</a></code></pre></div>
<p>She tests the function on simulated data, 10000 fake distances with mean 2 and standard deviation 2.7. Either a small miracle happened, or she got it right at first try:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" title="1"><span class="co">#simulated sample size</span></a>
<a class="sourceLine" id="cb323-2" title="2">n&lt;-<span class="dv">10000</span></a>
<a class="sourceLine" id="cb323-3" title="3"><span class="co"># simulated mean</span></a>
<a class="sourceLine" id="cb323-4" title="4">mG&lt;-<span class="dv">2</span></a>
<a class="sourceLine" id="cb323-5" title="5"><span class="co"># simulated standard deviation</span></a>
<a class="sourceLine" id="cb323-6" title="6">sdG&lt;-<span class="fl">0.7</span></a>
<a class="sourceLine" id="cb323-7" title="7"><span class="co"># simulated sample</span></a>
<a class="sourceLine" id="cb323-8" title="8">xs=<span class="kw">rnorm</span>(n,<span class="dt">mean=</span>mG,<span class="dt">sd=</span>sdG)</a>
<a class="sourceLine" id="cb323-9" title="9"><span class="co"># MLE of the parameters</span></a>
<a class="sourceLine" id="cb323-10" title="10">MLEGau&lt;-<span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">fn=</span>minuslogliknorm,<span class="dt">data=</span>xs)</a>
<a class="sourceLine" id="cb323-11" title="11">MLEGau</a></code></pre></div>
<pre><code>## $par
## [1] 1.9994644 0.6987295
## 
## $value
## [1] 10604.8
## 
## $counts
## function gradient 
##       65       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>Note she now has more than 1 parameter, so <code>optimize</code> was not an option, and had to use function <code>optim</code>. This requires starting values via argument <code>par</code>, but the rest is similar to <code>optimize</code>, the <code>data</code> and the function to maximize is <code>fn</code>. The output is a bit messier, and while the resulting object components are all relevant to know about, we ignore them for now for simplicity. If we evaluate this likelihood using a brute force grid approach, this is what we get</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" title="1"><span class="co">#to implement later</span></a></code></pre></div>
<p>Notice that this image reminds us of Boredomnesia! Now, we know how to do this for more than one parameter, but why would we. After all, if I wanted to estimate the MLE of a Gaussian, actually, the sample mean <code>mean(xs)</code>=1.999534 and the sample standard deviation <code>sd(xs)</code>=0.6987875 are what I want. So the above values obtained by <code>optim</code> for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the MLEs, 1.9994644 and 1.9994644 respectively, are really just numerical approximations of the real analytically obtainable MLE’s 1.999534 and 0.6987875, respectively (note: need, strictly, to refer to minor detail regarding denominator of the standard deviation, considering n, the MLE, or n-1, not MLE but unbiased; ME students can ignore detail for now!). These are themselves, in this case where we know reality, estimates of the true simulated values generating our data, 2 and 0.7, respectively. So all quite reasonable and close, really, what is not surprising given a sample size of 10^{4}!</p>
<p>Now, what about based on the distances Andreia had for the 5 nests</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb326-1" title="1">dists</a></code></pre></div>
<pre><code>## [1] 0.78 1.73 1.32 0.54 2.12</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" title="1">MLEGaud&lt;-<span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">fn=</span>minuslogliknorm,<span class="dt">data=</span>dists)</a>
<a class="sourceLine" id="cb328-2" title="2">MLEGaud</a></code></pre></div>
<pre><code>## $par
## [1] 1.2979771 0.5840272
## 
## $value
## [1] 4.406008
## 
## $counts
## function gradient 
##       53       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>Note that, as it might be interesting for later, that nests with eggs were closer to the water. But remember also that with such a small sample size, believing in that as being some indication of reality, rather than just a fluke, is a matter of faith.</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb330-1" title="1"><span class="kw">plot</span>(dists,nests,<span class="dt">ylab=</span><span class="st">&quot;nest with eggs&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Distance to water (km)&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.24-1.png" width="672" /></p>
<p>We get some estimates, despite the fact that it is yet unclear why we should do it this way and not just use <code>mean</code> and <code>sd</code>. To see why, we continue our story, and the plot thickens…</p>
</div>
<div id="the-case-of-a-linear-model" class="section level2">
<h2><span class="header-section-number">15.4</span> The case of a linear model</h2>
<p>We started by talking about <code>lm</code>, so, what is happening behind <code>lm</code>.</p>
<p>Let’s us imagine that for each nest, Andreia also related the size of the nest with the distance to the water. The size of the nest, diameter, in cm, was as below, She is interested in describing, modeling, explaining how nest size changes as a function of distance to the water.</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" title="1">size&lt;-<span class="kw">c</span>(<span class="dv">17</span>,<span class="fl">19.3</span>,<span class="fl">21.2</span>,<span class="fl">13.2</span>,<span class="dv">25</span>)</a></code></pre></div>
<p>We can visualize the relationship between the distance to the water and the nest diameter</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" title="1"><span class="kw">plot</span>(dists,size,<span class="dt">xlab=</span><span class="st">&quot;Distance (km)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Nest diameter, cm&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.26-1.png" width="672" /></p>
<p>So now Andreia needs a likelihood. Since the above relation seems linear, she remembers that the linear model is given by</p>
<p><span class="math display">\[y_i=a+b x_i+e_i\]</span></p>
<p>where the <span class="math inline">\(e_i\)</span> are a Gaussian with mean 0 and constant variance <span class="math inline">\(\sigma^2\)</span>. And then she has another epiphany and realizes that she can construct data for which a likelihood can be derived. Because if she rearranges the above, you have</p>
<p><span class="math display">\[e_i=y_i-(a+b x_i)=y_i-\hat y_i\]</span>
where, remember, <span class="math inline">\(e_i\)</span> are a Gaussian with mean 0 and constant variance <span class="math inline">\(\sigma^2\)</span>. And so we can build a likelihood that exploits that Gaussian density for the observed errors, as</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" title="1">liklm=<span class="cf">function</span>(pars,data){</a>
<a class="sourceLine" id="cb333-2" title="2">  <span class="co">#data must be a data.frame with columns y and x</span></a>
<a class="sourceLine" id="cb333-3" title="3">  a=pars[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb333-4" title="4">  b=pars[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb333-5" title="5">  sigma=pars[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb333-6" title="6">  ps=<span class="kw">dnorm</span>(data<span class="op">$</span>y<span class="op">-</span>(a<span class="op">+</span>b<span class="op">*</span>data<span class="op">$</span>x),<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span>sigma)</a>
<a class="sourceLine" id="cb333-7" title="7">  <span class="co">#minus loglik</span></a>
<a class="sourceLine" id="cb333-8" title="8">  loglik=<span class="op">-</span><span class="kw">sum</span>(<span class="kw">log</span>(ps))</a>
<a class="sourceLine" id="cb333-9" title="9">  <span class="kw">return</span>(loglik)</a>
<a class="sourceLine" id="cb333-10" title="10">}</a></code></pre></div>
<p>and we use it over our sample</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" title="1">lmMLE&lt;-<span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">fn=</span>liklm,<span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">y=</span>size,<span class="dt">x=</span>dists))</a>
<a class="sourceLine" id="cb334-2" title="2">lmMLE</a></code></pre></div>
<pre><code>## $par
## [1] 11.120181  6.178854  1.631731
## 
## $value
## [1] 9.542754
## 
## $counts
## function gradient 
##      192       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>and so we get as estimates of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> and <span class="math inline">\(\sigma\)</span> of 11.1201813, 6.1788541 and 1.6317308, respectively. So finally, Andreia uses her estimated values for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and puts them over the above plot of the data</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" title="1"><span class="kw">plot</span>(dists,size,<span class="dt">xlab=</span><span class="st">&quot;Distance (km)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Nest diameter, cm&quot;</span>)</a>
<a class="sourceLine" id="cb336-2" title="2"><span class="kw">abline</span>(lmMLE<span class="op">$</span>par[<span class="dv">1</span>],lmMLE<span class="op">$</span>par[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.29-1.png" width="672" /></p>
<p>Now, we know that <code>lm</code> does this kind of stuff very efficiently, so how do these compare across? We can look at the outcome of the <code>lm</code> call</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" title="1">lm0&lt;-<span class="kw">lm</span>(size<span class="op">~</span>dists)</a>
<a class="sourceLine" id="cb337-2" title="2"><span class="kw">summary</span>(lm0)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = size ~ dists)
## 
## Residuals:
##       1       2       3       4       5 
##  1.0616 -2.5101  1.9240 -1.2550  0.7794 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   11.117      2.296   4.843   0.0168 *
## dists          6.181      1.613   3.832   0.0313 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.106 on 3 degrees of freedom
## Multiple R-squared:  0.8304,	Adjusted R-squared:  0.7738 
## F-statistic: 14.68 on 1 and 3 DF,  p-value: 0.03132</code></pre>
<p>and when we add these to the above plot, we see we were bang on: the two lines are indistinguishable by eye.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" title="1"><span class="kw">plot</span>(dists,size,<span class="dt">xlab=</span><span class="st">&quot;Distance (km)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Nest diameter, cm&quot;</span>)</a>
<a class="sourceLine" id="cb339-2" title="2"><span class="kw">abline</span>(lmMLE<span class="op">$</span>par[<span class="dv">1</span>],lmMLE<span class="op">$</span>par[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb339-3" title="3"><span class="kw">abline</span>(lm0,<span class="dt">lty=</span><span class="dv">3</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;green&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.30.5-1.png" width="672" /></p>
<p>So, but still, why would we do it this way, since <code>lm</code> does it with less hassle, faster, and probably better? For a number of reasons, including:</p>
<ol style="list-style-type: decimal">
<li>because it allows us a framework that is generalizable to any model for which we can define the likelihood, so it works for more than standard regression models</li>
<li>because it allows us to really understand what is happening in the background, as an example, we can relate the profile of the likelihood to the variance around the parameter estimates</li>
</ol>
<p>So to complete this chapter, lets see an example for which a dedicated function like <code>lm</code> is not available off the shelf, and we would really need to write down our own likelihood to get meaningful ecological inferences.</p>
</div>
<div id="the-really-interesting-case" class="section level2">
<h2><span class="header-section-number">15.5</span> The really interesting case</h2>
<p>Imagine now that we were interested in relating the probability for a nest being successful with the distance of the nest to a body of water. A possible ecological explanation for there being a negative relationship between the distance and the success probability might be that near water bodies there are usual more insects, and hence more food which means improved body condition and hence incentives for attempting reproduction.</p>
<p>We could therefore hypothesize that</p>
<p><span class="math display">\[\theta_i=f(d_i)\]</span></p>
<p>and that this function is such that for a given nest <span class="math inline">\(i\)</span>, the larger the distance <span class="math inline">\(d_i\)</span> the smaller the probability of success <span class="math inline">\(\theta_i\)</span>.</p>
<p>A possible way to conceptualize that relationship might be by choosing <span class="math inline">\(f\)</span> in a way that forces <span class="math inline">\(\theta\)</span> to be in the plausible range for a probability. One such way is to assume a logistic relationship between the probability and the distance</p>
<p><span class="math display">\[\theta_i=\frac{1}{1+exp(-(\alpha+\beta d_i))}\]</span></p>
<p>which we could easily code up in R as</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" title="1">thetafd&lt;-<span class="cf">function</span>(alpha,beta,d){</a>
<a class="sourceLine" id="cb340-2" title="2">  res&lt;-<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>(alpha<span class="op">+</span>beta<span class="op">*</span>d)))</a>
<a class="sourceLine" id="cb340-3" title="3">  <span class="kw">return</span>(res)</a>
<a class="sourceLine" id="cb340-4" title="4">}</a></code></pre></div>
<p>We can visualize what that function might look like for some arbitrary values for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" title="1">alpha&lt;-<span class="fl">1.6</span></a>
<a class="sourceLine" id="cb341-2" title="2">beta &lt;-<span class="st"> </span><span class="fl">-1.8</span></a></code></pre></div>
<p>These values imply that the probability of a nest near a body of water having eggs is relatively high, in fact, if just by the water (<span class="math inline">\(d=0\)</span>) then around 0.83, and around 1km it will be 0.45 but by about 5km from the water the probability is down at effectively 0.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" title="1">alldists&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb342-2" title="2"><span class="kw">plot</span>(alldists,<span class="kw">thetafd</span>(alpha,beta,alldists),<span class="dt">ylab=</span><span class="st">&quot;P(nest with eggs)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Distance from water (km)&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.33-1.png" width="672" /></p>
<p>Having done this, then it is relatively simple to modify the likelihood for theta that we used for <span class="math inline">\(\theta\)</span>, to replace the <span class="math inline">\(\theta\)</span> by a function of the relevant covariate <span class="math inline">\(d\)</span></p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" title="1">loglikthetad=<span class="cf">function</span>(pars,data){</a>
<a class="sourceLine" id="cb343-2" title="2">  nests&lt;-data[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb343-3" title="3">  dists&lt;-data[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb343-4" title="4">  alpha&lt;-pars[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb343-5" title="5">  beta&lt;-pars[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb343-6" title="6">  theta&lt;-<span class="kw">thetafd</span>(alpha,beta,dists)</a>
<a class="sourceLine" id="cb343-7" title="7">  minusloglik=<span class="op">-</span><span class="kw">sum</span>(<span class="kw">log</span>(theta<span class="op">^</span><span class="kw">sum</span>(nests<span class="op">==</span><span class="dv">1</span>))<span class="op">+</span><span class="kw">sum</span>(<span class="kw">log</span>((<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span><span class="kw">sum</span>(nests<span class="op">==</span><span class="dv">0</span>))))</a>
<a class="sourceLine" id="cb343-8" title="8">  <span class="kw">return</span>(minusloglik)</a>
<a class="sourceLine" id="cb343-9" title="9">}</a></code></pre></div>
<p>and so we could maximize this likelihood based on the data, both the successes and the distances to the water observed above</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb344-1" title="1">thetadMLE&lt;-<span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">fn=</span>loglikthetad,<span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">nests=</span>nests,<span class="dt">dists=</span>dists))</a>
<a class="sourceLine" id="cb344-2" title="2">thetadMLE</a></code></pre></div>
<pre><code>## $par
## [1] -2.0139580650 -0.0006849422
## 
## $value
## [1] 30.7879
## 
## $counts
## function gradient 
##       81       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>Now, this provides an interesting ecological insight. Unlike the <code>naive</code> estimator for the probability of a nest having eggs we obtained above, of 0.4, we now can relate that probability to the distance from the water. And what we see is that the probability of a random nest having nests might be well below that. We just happened to consider a small sample for which two out of five nests were near the water. While the <code>naive</code> estimate might change considerably depending on the relative amount of nests near the water versus nests away from the water, if the main determinant of nest success was the distance from the water, our final model would allow a better estimate of the true probability of success of a nest.</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb346-1" title="1">alldists&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb346-2" title="2"><span class="kw">plot</span>(alldists,<span class="kw">thetafd</span>(thetadMLE<span class="op">$</span>par[<span class="dv">1</span>],thetadMLE<span class="op">$</span>par[<span class="dv">2</span>],alldists),<span class="dt">ylab=</span><span class="st">&quot;Estimated P(nest with eggs)&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Distance from water (km)&quot;</span>)</a></code></pre></div>
<p><img src="ECOMODbook_files/figure-html/ch13.36-1.png" width="672" /></p>
</div>
<div id="likelihood-above-and-beyond" class="section level2">
<h2><span class="header-section-number">15.6</span> Likelihood, above and beyond</h2>
<p>The examples presented in this chapter should illustrate a couple of fundamental points regarding likelihoods.</p>
<p>The likelihood is a concept that allows one to obtain parameter estimates for model parameters that might allow ecological insights, provided that the model parameters are interpretable</p>
<p>The likelihood allows to establish a full framework that can be extended an generalizable to become as complicated as a researcher might want. However, if you complicate the model enough, not surprisingly, the likelihood might become hard to write down and even harder to maximize, and then one needs to find alternatives.</p>
<p>One such alternative might actually be to change the inferential framework. The likelihood is actually the basis of the Bayesian inferential paradigm, but there you combine the information from the likelihood with a prior to provide a posterior distribution from which inferences can be made. The prior will represent previous knowledge about unknown quantities, like the parameters in a model. That opens a world of possibilities, like the possibility of including information from previous studies to make more reliable inferences based on the data collected in the current study. is a story for another book, however.</p>
<p>Most of the statistical methods that you have used are probably based on a likelihood. That is the case for most parametric statistical tests like t-tests and ANOVA’s (despite the fact that ANOVA’s are typically implemented not exploiting the likelihood but using decompositions of the sources of variation into sums of squares associated to different components of the underlying linear models. That was a story we touch upon briefly, from a different perspective (see chapter 8).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conlusion-on-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="aula14.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ECOMODbook.pdf", "ECOMODbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
